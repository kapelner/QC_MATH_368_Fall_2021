\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 368/621 Fall \the\year{} Homework \#3}

\author{Professor Adam Kapelner} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due by email 11:59PM October 30, \the\year{} \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, review from math 241 about conditional probability, expectation and variance then read on your own about the Skellam distribution, the exponential, Erlang, the Poisson Process, the gamma family of functions, discrete and continuous transformations, quantile functions, the Pareto I distribution, the Laplace distribution, the Logistic distribution.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 7 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}

\problem{We will now go over the Skellam distribution. In class we derived the PMF of $D = X_1 - X_2$ where $X_1, X_2 \iid \poisson{\lambda}$. This was first published in 1937. This was a special case of general Skellam distribution which is defined below:

\beqn
D \sim \text{Skellam}(\lambda_1, \lambda_2) := e^{-(\lambda_1 + \lambda_2)} \tothepow{\frac{\lambda_1}{\lambda_2}}{d/2} I_{|d|}(2 \sqrt{\lambda_1\lambda_2})
\eeqn

\noindent where $I_x(a)$ denotes the \href{https://en.wikipedia.org/wiki/Bessel_function\#Modified_Bessel_functions}{modified Bessel function of the first kind} defined as:

\beqn
I_{\alpha}(\lambda) := \sum_{x = 0}^\infty \frac{\tothepow{\overtwo{\lambda}}{2x + \alpha}}{x! (x + \alpha)!}
\eeqn

\noindent when $\alpha \in \naturals_0$.
}\vspace{0.5cm}

\begin{enumerate}

\easysubproblem{Show that for $\lambda_1 = \lambda_2$, we get the formula derived in class i.e. the special case of the Skellam derived in 1937.}\spc{1}


\intermediatesubproblem{[MA] If $D = X_1 - X_2$ where $X_1 \sim \poisson{\lambda_1}$ independent of $X_2 \sim \poisson{\lambda_2}$, show that $D \sim \text{Skellam}(\lambda_1, \lambda_2)$.}\spc{10}


\easysubproblem{The Yankees play the Mets. Assume the number of runs scored is Poisson with rate parameter $\lambda_Y = 7$ for the Yankees and rate parameter $\lambda_M = 5$ for the Mets. Assume the number of runs scored by the Yankees is independent of the number of runs scored by the Mets. What score difference is expected in this baseball game?}\spc{2}

\easysubproblem{Find the probability the Mets beat the Yankees by 3. Leave in notation. Do not compute an actual number.}\spc{2}

%\easysubproblem{[MA] Figure out a way to compute the probability in the previous question to two significant figures.}\spc{1}


\easysubproblem{Write an expression to compute the probability the Mets beat the Yankees. Leave in notation. Do not compute an actual number.}\spc{1}

\end{enumerate}


\problem{These exercises will introduce continuous convolutions.}


\begin{enumerate}

\intermediatesubproblem{In the space on the left, explain when you would employ each of these five expressions for the convolution of PDF's for two continuous r.v.'s:

\vspace{-0.5cm}
\beqn
f_{X+Y}(t) = f_X(x) * f_Y(x) &&\\
&:=& \int\displaylimits_\reals f_{X,Y}(x, t-x) dx \\
&=& \int\displaylimits_\reals f_X(x) f_Y(t-x) dx \\
&=& \int\displaylimits_{\support{X}} f_X(x) f_Y(t-x) \indic{t-x \in \support{Y}}dx \\
&=& \int\displaylimits_\reals f(x) f(t-x) dx \\
&=& \int\displaylimits_{\support{X}} f(x) f(t-x) \indic{t-x \in \support{X}}dx
\eeqn}\spc{0}

\easysubproblem{If $X_1, X_2 \iid \uniform{0}{1}$, find $\support{T}$ where $T = X_1 + X_2$.}\spc{0}

\intermediatesubproblem{Find $f_T(t)$ using the convolution formula method.}\spc{5}

\easysubproblem{If $X_1 \sim \uniform{a_1}{b_1}$ and $X_2 \sim \uniform{a_2}{b_2}$ independently, find $\support{T}$ where $T = X_1 + X_2$.}\spc{0}

\hardsubproblem{Find $f_T(t)$ using either the CDF method or the convolution formula method.}\spc{15}~\vspace{1cm}


\end{enumerate}


\problem{This question reviews the Exponential distribution and introduces the Erlang distribution.}

\begin{enumerate}

\easysubproblem{Derive the Exponential from the Geometric r.v. as we did in class. Find its CDF, PMF and PDF. Make sure the CDF, PMF and PDF are valid $\forall x \in \reals$. Illustrate the PDF and CDF.}\spc{10}


\easysubproblem{Let $X_1, \ldots, X_k \iid \exponential{\lambda}$. Show that the sum $T = X_1 + \ldots + X_k \sim \erlang{k}{\lambda}$ by finding its PDF.}\spc{10}



\easysubproblem{Find the expectation of $T \sim \erlang{k}{\lambda}$. Use the linearity rules!}\spc{1}

\easysubproblem{Find the variance of $T \sim \erlang{k}{\lambda}$. Use the linearity rules!}\spc{3}

\easysubproblem{Why is the geometric distribution analogous to the exponential distribution? Why is the negative binomial distribution analogous to the erlang distribution?}\spc{3}

\easysubproblem{If the length of time (in minutes) of a phone call is distributed $\exponential{0.5}$ and all phone call lengths are $\iid$, find an expression for the probability that the total sum time of 37 phone calls lasts longer than 17 minutes.}\spc{2}


\intermediatesubproblem{Show that for any exponential r.v. with rate parameter $\lambda$ the distribution is \qu{memoryless} meaning that for any $c$, a positive constant, $\cprob{X > x + c}{X > c} = \prob{X > x}$.}\spc{5}

\end{enumerate}

\problem{These exercises will give you practice with the gamma function.}


\begin{enumerate}

\easysubproblem{Write the definition of $\Gamma\parens{x}$.}\spc{1}

\intermediatesubproblem{Prove $\Gamma\parens{x + 1} = x \Gamma\parens{x}$.}\spc{4}


\intermediatesubproblem{Write the definition of $Q\parens{x,a}$ without using the gamma function.}\spc{2}


\intermediatesubproblem{If $0 < a < b < \infty$, find an integral expression for $\Gamma\parens{x, b} - \gamma\parens{x, a}$.}\spc{2}


\easysubproblem{For $a,c \in (0, \infty)$, prove the following:

\beqn
\int_a^\infty t^{x-1} e^{-ct} dt = \frac{\Gamma\parens{x, ac}}{c^x}
\eeqn}\spc{6}


\easysubproblem{Write the PMF's and parameter spaces of both the extended negative binomial rv and the negative binomial rv model. Explain how the latter \qu{upgrades} the former.}\spc{3}

\easysubproblem{Write the PDF's and paramter spaces of both the gamma rv and the Erlang rv model. Explain how the latter \qu{upgrades} the former.}\spc{3}

\end{enumerate}

\problem{These exercises will give you practice with the Poisson process and the analogous Binomial-Negative Binomial relationship.}


\begin{enumerate}

\easysubproblem{Write the assumptions and the main result of the Poisson process (an equivalence of two probability statements and then an equivalence using the CDF's of the Erlang and the Poisson models).} \spc{5}

\easysubproblem{Assume $X_1, X_2, X_3, \ldots \iid \exponential{\lambda}$. Calculate $\prob{X_1 + X_2 + X_3 + X_4 + X_5 < 1}$ using the two different ways (i.e. via the Poisson Process relationship).}\spc{5}

\intermediatesubproblem{Let $N \sim \poisson{\lambda}$. Describe a way to use the realizations from the r.v.'s $X_1, X_2, X_3, \ldots \iid \exponential{\lambda}$ to create a realization $n$ from the Poisson model.}\spc{2}

\hardsubproblem{Assume $X_1, X_2, X_3, \ldots \iid \exponential{\lambda}$. Calculate $\prob{X_1 + X_2 + X_3 + X_4 + X_5  < m}$ where $m \in \naturals$ using two different ways (i.e. via the Poisson Process relationship).}\spc{6}

\extracreditsubproblem{Prove the analgous Binomial-Negative Binomial relationship (an equivalence of two probability statements and then an equivalence using the CDF's of the Binomial and the Negative Binomial models).}\spc{10}



\end{enumerate}

\problem{These exercises will give you practice with transformations of discrete r.v.'s.}


\begin{enumerate}

\easysubproblem{Let $X \sim \binomial{n}{p}$. Find the PMF of $Y = g(X) = \natlog{X + 1}$.}\spc{3}

\intermediatesubproblem{Let $X \sim \binomial{n}{p}$. Find the PMF of $Y = g(X) = X^2$. Is $g(X)$ monotonic? Does that matter for this r.v.?}\spc{2}

\hardsubproblem{Let $X \sim \binomial{n}{p}$ where $n$ is an even number. Find the PMF of $Y = g(X) = \text{mod}(X, 2)$ where \qu{mod} denotes modulus division of the first argument by the second argument.}\spc{5}

\hardsubproblem{[MA] Let $X \sim \negbin{k}{p}$. Find the PMF of $Y = g(X) = \text{mod}(X, n)$ where $n \in \naturals$.}\spc{7}
\end{enumerate}

\problem{These exercises will give you practice with transformations of continuous r.v.'s and the quantile function.}

\begin{enumerate}

\intermediatesubproblem{Let $X \sim \stduniform$. Find the PDF of $Y = g(X) = aX + c$. Make sure you're careful with the indicator function that specifies the support. There are two cases.}\spc{3}

\intermediatesubproblem{Let $X \sim \exponential{\lambda}$. Find the PDF of $Y = g(X) = \natlog{X}$.}\spc{3}

\extracreditsubproblem{Let $X \sim \exponential{\lambda}$. Find the PDF of $Y = g(X) = \sin{X}$.}\spc{3}

\intermediatesubproblem{Let $X \sim \stduniform$. Find the PDF of $Y = g(X) = \natlog{\frac{X}{1 - X}}$. If this is a brand name r.v., mark it so and include its parameter values.}\spc{5}


\easysubproblem{Find the Quantile function of $X$ where $X \sim \text{Logistic}(0, 1)$.}\spc{3}

\easysubproblem{Find the PDF of $Y = \sigma X + \mu \sim \text{Logistic}(\mu, \sigma)$ where $X \sim \text{Logistic}(0, 1)$.}\spc{3}

\hardsubproblem{Let $X \sim \text{Logistic}(0,1)$. Find the PDF of $Y = g(X) = \oneover{1 + e^{-X}}$. If this is a brand name r.v., mark it so and include its parameter values.}\spc{5}

\intermediatesubproblem{Let $X \sim \exponential{\lambda}$. Find the PDF of $Y = g(X) = ke^X$ where $k>0$. This will be a brand name r.v., so mark it so and include its parameter values.}\spc{6}

\easysubproblem{Rederive the $X \sim \text{Laplace}(0, 1)$ r.v. model by taking the difference of two standard exponential r.v.'s.}\spc{8}

\easysubproblem{Let $X \sim \text{Laplace}(0, 1)$. Prove that $\expe{X} = 0$ without using the integral definition. There's a trick.}\spc{3}

\easysubproblem{Find the PDF of $Y = \sigma X + \mu \sim \text{Laplace}(\mu, \sigma)$ where $X \sim \text{Laplace}(0, 1)$.}\spc{3}

\hardsubproblem{Show that $\mathcal{E} \sim \text{Laplace}(0, \sigma)$ is a reasonable error distribution.}\spc{4}

\intermediatesubproblem{[MA] Find the Quantile function of $X$ where $X \sim \text{Laplace}(0, 1)$.}\spc{4}

%\intermediatesubproblem{[MA] Prove that $X \sim \text{ParetoI}(1, log_4(5))$ models the \qu{Pareto Principle}.}\spc{10}

%\intermediatesubproblem{Is $X \sim \text{ParetoI}(1, log_4(5))$ a good model for land ownership amount for individuals? Why / why not?}\spc{4}

\hardsubproblem{[MA] Let $X \sim \text{ParetoI}(k, \lambda)$. Show that $Y = X~|~X > c$ where $c > k$ is also a ParetoI r.v. and find its parameter values.}\spc{7}

%\extracreditsubproblem{[MA] Prove or disprove that considering any ParetoI conditional on being larger than a certain value would also be a power rule where the top $q$ proportion of the unit modeled is in the hands of the top $\bar{q} := 1 - q$ values of the unit. 
%
%This is the whole idea of a power law e.g. if the top 1\% of the country owns 99\% of the wealth then the power law also implies that of the top 1\% of the top 1\% of them owns 99\% of that 99\% and etc.
%
%I couldn't seem to get it in one hour of trying so it is likely hard to show this. Or maybe it's incorrect. You get EC if you make a good effort.}\spc{5}

\end{enumerate}

\end{document}

